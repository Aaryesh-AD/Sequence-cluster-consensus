{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Sequence Cluster Analysis: \n",
    "### **For Retrieval of cluster representative sequence and Generation of Consensus Sequences from the clustered sequence.**\n",
    "\n",
    "Author: Aaryesh Deshpande\n",
    "\n",
    "## Overview\n",
    "This notebook explores various clustering methods and generates consensus sequences from clustered groups. The goal is to analyze sequences, group them effectively, and retrieve representative sequences for further simulations.\n",
    "\n",
    "---\n",
    "\n",
    "## Methods Implemented\n",
    "### Clustering Algorithms:\n",
    "- **K-Means Clustering**: Partitions data into `k` clusters by minimizing the variance within each cluster.\n",
    "- **Hierarchical Clustering**: Builds nested clusters by either merging or splitting them successively (Agglomerative Clustering).\n",
    "- **BIRCH Clustering**: Constructs a tree structure for clustering large datasets efficiently.\n",
    "- **DBSCAN Clustering**: Density-based algorithm that groups points closely packed together and identifies noise points.\n",
    "- **OPTICS Clustering**: Similar to DBSCAN but can identify clusters of varying densities.\n",
    "- **HDBSCAN**: Hierarchical density-based clustering to find clusters with variable densities.\n",
    "- **Gaussian Mixture Model**: Assumes data points are generated from a mixture of Gaussian distributions.\n",
    "\n",
    "### Feature Extraction:\n",
    "Feature vectors are created using a combined approach:\n",
    "1. **Sequence K-Mers**: Extract subsequences of length `k` from protein or DNA sequences to represent sequence patterns.\n",
    "2. **Pseudo Amino Acid Composition (PAAC)**: Encodes sequences based on physicochemical properties:\n",
    "    - **Hydrophobicity**\n",
    "    - **Hydrophilicity**\n",
    "    - **Mass**\n",
    "\n",
    "### Representative Sequence Retrieval\n",
    "#### Most Representative Sequence:\n",
    "- The sequence closest to the cluster medoid is identified as the most representative sequence.\n",
    "\n",
    "#### Consensus Sequences:\n",
    "- Consensus sequences are generated for each cluster by aligning all sequences within the group and selecting the most frequent residues at each position using multiple sequence alignment generated by CLUSTAL and consensus generated by biopython.\n",
    "---\n",
    "\n",
    "## Applications:\n",
    "- Functional annotation of proteins\n",
    "- Reducing dataset size for computational simulations like Molecular Dynamics simulation.\n",
    "- Identifying key representative sequences for downstream analysis\n",
    "\n",
    "---\n",
    "\n",
    "### Usage Instructions:\n",
    "- Replace the file paths in the code with your desired input and output paths.\n",
    "- Look for comments within the code to locate the relevant sections for modifying the file paths.\n",
    "\n",
    "For detailed implementations, refer to the sections with Python code and comments. Each clustering algorithm and its corresponding feature extraction methods are described step-by-step. Each step is commented for clarity.\n",
    "\n",
    "We welcome contributions to improve this project! Whether you've found a bug, have suggestions for fixes, or would like to add new features, your input is greatly appreciated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing FASTA File: Renaming and Tidying Headers\n",
    "\n",
    "This section of the code focuses on tidying up the headers in a given FASTA file by renaming them in a systematic and consistent format. The renaming process starts with an anchor sequence named `sample_0` (the first sequence) and continues sequentially as `sample_01`, `sample_02`, ..., `sample_XX`.\n",
    "\n",
    "#### Key Steps:\n",
    "1. **Read and Parse the FASTA File**:\n",
    "   - Input the original FASTA file containing sequences with varied or inconsistent headers.\n",
    "   \n",
    "2. **Rename Headers**:\n",
    "   - Assign new headers in the format `sample_XX`, where `XX` represents the sequence number.\n",
    "   - Ensure the first sequence is labeled `sample_0`.\n",
    "\n",
    "3. **Save the Processed FASTA File**:\n",
    "   - Write the updated sequences with tidy headers into a new FASTA file for downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read FASTA formatted file and return sequences as a list\n",
    "def read_fasta(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        fasta_data = file.read().split(\">\")[1:]\n",
    "    sequences = {}\n",
    "    for lines in fasta_data:\n",
    "        header_key = f\"Sample_{len(sequences)}\"\n",
    "        seq_value = lines.split(\"\\n\", 1)[1].replace(\"\\n\", \"\").strip()\n",
    "        sequences[header_key] = seq_value\n",
    "    return sequences\n",
    "    \n",
    "#Replace with your FASTA file path and output file path\n",
    "# Define Fasta File Path\n",
    "fasta_file_path = \"path/to/your/fasta_file.fa\" # Change this to your FASTA file path\n",
    "output_merge_path = \"merged_fasta.fa\" # Change this to your desired output file path\n",
    "\n",
    "# Read the FASTA file and store the sequences in a dictionary\n",
    "reader_obj = read_fasta(fasta_file_path)\n",
    "reader_obj_values = list(reader_obj.values())\n",
    "\n",
    "# Write the reader_obj_values sequences to a new FASTA file\n",
    "with open(output_merge_path, 'w') as output_file:\n",
    "    for idx, seq in enumerate(reader_obj_values):\n",
    "        output_file.write(f\">Sample_{idx}\\n{seq}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Imports\n",
    "\n",
    "This cell contains all the necessary module imports for the script, including standard Python libraries, third-party libraries, and specialized packages for bioinformatics and clustering.\n",
    "\n",
    "#### Requirements:\n",
    "Ensure the following libraries are installed in your environment before running this script. The required libraries can be installed using the following commands:\n",
    "\n",
    "- **Using Conda**:\n",
    "  ```bash\n",
    "  conda install numpy pandas matplotlib biopython scikit-learn hdbscan scipy\n",
    "  ```\n",
    "- **Using pip**:\n",
    "    ```bash\n",
    "    pip install numpy pandas matplotlib biopython scikit-learn hdbscan scipy\n",
    "    ```\n",
    "\n",
    "- You need to also have clustio installed on your system for running the CLUSTALW command. To install clustio, use the following command:\n",
    "    ```bash\n",
    "    sudo apt install clustio (for linux)\n",
    "    ```\n",
    "To install all of the dependencies of this environment to run the script, use the ```bash environment_cluster.yml``` file to setup and install dependencies to a conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default python libraries\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 3rd party libraries\n",
    "# Standard python libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Biopython libraries\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "from Bio.Align.AlignInfo import SummaryInfo\n",
    "from Bio.Align.Applications import ClustalOmegaCommandline \n",
    "\n",
    "# Clustering libraries\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, Birch, AgglomerativeClustering, OPTICS\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "# Warning messages suppressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mer Feature Extraction\n",
    "\n",
    "This section focuses on generating sequence-based features using **K-mer extraction**. K-mers are subsequences of length `k` that are derived from a given sequence. The length of each k-mer is defaulted to 3, you can change that below to your own value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-mer feature extraction\n",
    "def kmer_count(sequence, k=3):\n",
    "    kmer_dict = {}\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmer = sequence[i:i+k]\n",
    "        kmer_dict[kmer] = kmer_dict.get(kmer, 0) + 1\n",
    "    return kmer_dict\n",
    "\n",
    "# Generate k-mer matrix\n",
    "def generate_kmer_matrix(sequences, k=3):\n",
    "    kmer_matrices = []\n",
    "    for sequence in sequences:\n",
    "        kmer_matrices.append(kmer_count(sequence, k))\n",
    "    return kmer_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Amino Acid Composition (PAAC) Feature Extraction\n",
    "\n",
    "This section extracts feature vectors based on Pseudo Amino Acid Composition (PAAC). The method involves calculating theta values to encode protein sequences effectively, enabling clustering and functional annotation.\n",
    "\n",
    "The implementation is adapted from the work of **Rakesh Busi**. The original repository for PAAC implementation in clustering can be found [here](https://github.com/RakeshBusi/Clustering.git).\n",
    "\n",
    "#### Reference\n",
    "If you use this implementation, please cite the following article:\n",
    "\n",
    "**Busi, Rakesh, Machingal, Pranav, Hemachandra, Nandyala, & Balaji, Petety V.**  \n",
    "*How suitable are clustering methods for functional annotation of proteins?*  \n",
    "bioRxiv, 2024.  \n",
    "Publisher: Cold Spring Harbor Laboratory.  \n",
    "DOI: [10.1101/2024.12.26.630370](https://doi.org/10.1101/2024.12.26.630370)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAAC feature extraction\n",
    "class PseudoAminoAcidComposition:\n",
    "    def __init__(self, fasta_data, lambda_=30, w=0.05):\n",
    "        self.lambda_ = lambda_\n",
    "        self.w = w\n",
    "        self.physicochemical_properties = {\n",
    "            'hydrophobicity': {'A': 0.62, 'C': 0.29, 'D': -0.90, 'E': -0.74, 'F': 1.19, 'G': 0.48, 'H': -0.40, 'I': 1.38, 'K': -1.50, 'L': 1.06, 'M': 0.64, 'N': -0.78, 'P': 0.12, 'Q': -0.85, 'R': -2.53, 'S': -0.18, 'T': -0.05, 'V': 1.08, 'W': 0.81, 'Y': 0.26},\n",
    "            'hydrophilicity': {'A': -0.5, 'C': -1.0, 'D': 3.0, 'E': 3.0, 'F': -2.5, 'G': 0.0, 'H': -0.5, 'I': -1.8, 'K': 3.0, 'L': -1.8, 'M': -1.3, 'N': 0.2, 'P': 0.0, 'Q': 0.2, 'R': 3.0, 'S': 0.3, 'T': -0.4, 'V': -1.5, 'W': -3.4, 'Y': -2.3},\n",
    "            'mass': {'A': 15.0, 'C': 47.0, 'D': 59.0, 'E': 73.0, 'F': 91.0, 'G': 1.0, 'H': 82.0, 'I': 57.0, 'K': 73.0, 'L': 57.0, 'M': 75.0, 'N': 58.0, 'P': 42.0, 'Q': 72.0, 'R': 101.0, 'S': 31.0, 'T': 45.0, 'V': 43.0, 'W': 130.0, 'Y': 107.0}\n",
    "        }\n",
    "        self.amino_acids = {idx + 1: aa for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
    "        self.collect_features(fasta_data)\n",
    "\n",
    "    # Collect features from the given FASTA data\n",
    "    def collect_features(self, fasta_data):\n",
    "        features = []\n",
    "        features.append(['#'] + list(self.amino_acids.values()) + [f'λ{i}' for i in range(1, self.lambda_ + 1)])\n",
    "        for i in range(0, len(fasta_data), 2):\n",
    "            accession = fasta_data[i].strip()\n",
    "            sequence = fasta_data[i + 1].strip()\n",
    "            if len(sequence) < self.lambda_:\n",
    "                print(f\"Sequence {accession} skipped due to insufficient length.\")\n",
    "                continue\n",
    "            paac = self.calculate_paac(sequence)\n",
    "            features.append([accession] + paac)\n",
    "        self.df = pd.DataFrame(features[1:], columns=features[0])\n",
    "\n",
    "    # Calculate pseudo amino acid composition\n",
    "    def calculate_paac(self, sequence):\n",
    "        lower_theta = self.calculate_lower_theta(sequence)\n",
    "        denominator = 1 + (self.w * sum(lower_theta.values()))\n",
    "        paac = []\n",
    "        for i in range(1, 21 + self.lambda_):\n",
    "            if i <= 20:\n",
    "                numerator = sequence.count(self.amino_acids[i]) / len(sequence)\n",
    "                paac.append(numerator / denominator)\n",
    "            else:\n",
    "                numerator = self.w * lower_theta[i - 20]\n",
    "                paac.append(numerator / denominator)\n",
    "        return paac\n",
    "\n",
    "    # Calculate lower theta values\n",
    "    def calculate_lower_theta(self, sequence):\n",
    "        lower_theta = {}\n",
    "        for i in range(1, self.lambda_ + 1):\n",
    "            if len(sequence) <= i:\n",
    "                lower_theta[i] = 0\n",
    "            else:\n",
    "                lower_theta[i] = (1 / (len(sequence) - i)) * self.calculate_upper_theta(sequence, i)\n",
    "        return lower_theta\n",
    "\n",
    "    # Calculate upper theta values\n",
    "    def calculate_upper_theta(self, sequence, i):\n",
    "        upper_theta = []\n",
    "        for j in range(len(sequence) - i):\n",
    "            diff = [\n",
    "                (self.physicochemical_properties[prop][sequence[j]] -\n",
    "                 self.physicochemical_properties[prop][sequence[j + i]]) ** 2\n",
    "                for prop in self.physicochemical_properties\n",
    "            ]\n",
    "            upper_theta.append(sum(diff) / len(self.physicochemical_properties))\n",
    "        return sum(upper_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Feature Sets: K-mers and PAAC\n",
    "\n",
    "This section combines **K-mer features** and **Pseudo Amino Acid Composition (PAAC)** features for enhanced feature representation. By leveraging both sequence patterns and physicochemical properties, this approach improves the quality of input features for clustering and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequences\n",
    "def load_sequences(file_path):\n",
    "    return [str(record.seq) for record in SeqIO.parse(file_path, \"fasta\")]\n",
    "\n",
    "# Combine K-mer and PAAC features\n",
    "def combine_features(sequences, k=3, lambda_=30, w=0.05):\n",
    "    kmer_features = generate_kmer_matrix(sequences, k)\n",
    "    \n",
    "    fasta_data = []\n",
    "    for idx, seq in enumerate(sequences):\n",
    "        fasta_data.extend([f\">seq_{idx}\", seq])\n",
    "    \n",
    "    paac = PseudoAminoAcidComposition(fasta_data, lambda_, w)\n",
    "    paac_features = paac.df.iloc[:, 1:].values.tolist()\n",
    "    \n",
    "    combined_features = []\n",
    "    max_length = max(len(list(kmer.values()) + paac) for kmer, paac in zip(kmer_features, paac_features))\n",
    "    \n",
    "    for kmer, paac in zip(kmer_features, paac_features):\n",
    "        feature = list(kmer.values()) + paac\n",
    "        # Pad with zeros if necessary\n",
    "        feature += [0] * (max_length - len(feature))\n",
    "        combined_features.append(feature)\n",
    "    return np.array(combined_features, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Methods and Evaluation\n",
    "\n",
    "This section implements multiple clustering algorithms to group sequences based on extracted features. Each clustering method is evaluated using the Silhouette Score to determine the quality of clustering.\n",
    "\n",
    "#### Clustering Functions:\n",
    "1. **`kmeans_clustering`**: Performs K-means clustering.\n",
    "2. **`dbscan_clustering`**: Density-based clustering using DBSCAN.\n",
    "3. **`hierarchical_clustering`**: Hierarchical clustering with Ward's linkage.\n",
    "4. **`birch_clustering`**: Efficient clustering using BIRCH.\n",
    "5. **`agglomerative_clustering`**: Agglomerative hierarchical clustering.\n",
    "6. **`optics_clustering`**: Clustering with OPTICS for variable density clusters.\n",
    "7. **`hdbscan_clustering`**: Hierarchical density-based clustering (HDBSCAN), optimized to return the top 10 clusters by size.\n",
    "\n",
    "#### Additional Features:\n",
    "- **Evaluation Function**: `evaluate_clustering` computes the Silhouette Score for each clustering result.\n",
    "- **Dimensionality Reduction**: PCA is applied to reduce the feature dimensions, improving clustering performance.\n",
    "- **Best Clustering Selection**: The method with the highest Silhouette Score is selected as the best result. Silhouette Coefficient (SC): Measures how similar an object is to its own cluster compared to other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering functions\n",
    "def kmeans_clustering(features, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    return labels\n",
    "\n",
    "def dbscan_clustering(features, eps, min_samples):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(features)\n",
    "    return labels\n",
    "\n",
    "def hierarchical_clustering(features, n_clusters):\n",
    "    linkage_matrix = linkage(features, method='ward')\n",
    "    labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "    return labels\n",
    "\n",
    "def birch_clustering(features, n_clusters):\n",
    "    birch = Birch(n_clusters=n_clusters)\n",
    "    labels = birch.fit_predict(features)\n",
    "    return labels\n",
    "\n",
    "def agglomerative_clustering(features, n_clusters):\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    labels = agglomerative.fit_predict(features)\n",
    "    return labels\n",
    "\n",
    "def optics_clustering(features, min_samples, xi, min_cluster_size):\n",
    "    optics = OPTICS(min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)\n",
    "    labels = optics.fit_predict(features)\n",
    "    return labels\n",
    "\n",
    "def hdbscan_clustering(features, min_cluster_size, min_samples):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "    labels = clusterer.fit_predict(features)\n",
    "    \n",
    "    # Count the occurrences of each label, excluding noise points (-1)\n",
    "    unique_labels, counts = np.unique(labels[labels != -1], return_counts=True)\n",
    "    \n",
    "    # Sort clusters by size in descending order\n",
    "    sorted_clusters = sorted(zip(unique_labels, counts), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select the top 10 clusters (or all if there are fewer than 10)\n",
    "    top_clusters = sorted_clusters[:min(10, len(sorted_clusters))]\n",
    "    top_cluster_labels = [label for label, _ in top_clusters]\n",
    "    \n",
    "    # Create a new label array with only the top clusters\n",
    "    new_labels = np.full_like(labels, -1)\n",
    "    for new_label, (old_label, _) in enumerate(top_clusters):\n",
    "        new_labels[labels == old_label] = new_label\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_clustering(features, labels):\n",
    "    if len(set(labels)) > 1 and -1 not in set(labels):\n",
    "        return silhouette_score(features, labels)\n",
    "    else:\n",
    "        return -1  # Invalid clustering (single cluster or noise)\n",
    "\n",
    "# Main clustering function\n",
    "def cluster_sequences(sequences, k=3, lambda_=30, w=0.05, n_clusters=10):\n",
    "    # Extract combined features\n",
    "    features = combine_features(sequences, k, lambda_, w)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    pca = PCA(n_components=min(50, len(normalized_features[0])))\n",
    "    reduced_features = pca.fit_transform(normalized_features)\n",
    "    \n",
    "    # Perform clustering\n",
    "    kmeans_labels = kmeans_clustering(reduced_features, n_clusters)\n",
    "    dbscan_labels = dbscan_clustering(reduced_features, eps=0.5, min_samples=5)\n",
    "    hierarchical_labels = hierarchical_clustering(reduced_features, n_clusters)\n",
    "    birch_labels = birch_clustering(reduced_features, n_clusters)\n",
    "    agglomerative_labels = agglomerative_clustering(reduced_features, n_clusters)\n",
    "    optics_labels = optics_clustering(reduced_features, min_samples=5, xi=0.05, min_cluster_size=int(0.05 * len(sequences)))\n",
    "    hdbscan_labels = hdbscan_clustering(reduced_features, min_cluster_size=5, min_samples=5)\n",
    "    \n",
    "    # Evaluate clustering results\n",
    "    kmeans_score = evaluate_clustering(reduced_features, kmeans_labels)\n",
    "    dbscan_score = evaluate_clustering(reduced_features, dbscan_labels)\n",
    "    hierarchical_score = evaluate_clustering(reduced_features, hierarchical_labels)\n",
    "    birch_score = evaluate_clustering(reduced_features, birch_labels)\n",
    "    agglomerative_score = evaluate_clustering(reduced_features, agglomerative_labels)\n",
    "    optics_score = evaluate_clustering(reduced_features, optics_labels)\n",
    "    hdbscan_score = evaluate_clustering(reduced_features, hdbscan_labels)\n",
    "    \n",
    "    print(f\"K-means Silhouette Score: {kmeans_score:.3f}\")\n",
    "    print(f\"DBSCAN Silhouette Score: {dbscan_score:.3f}\")\n",
    "    print(f\"Hierarchical Silhouette Score: {hierarchical_score:.3f}\")\n",
    "    print(f\"Birch Silhouette Score: {birch_score:.3f}\")\n",
    "    print(f\"Agglomerative Silhouette Score: {agglomerative_score:.3f}\")\n",
    "    print(f\"OPTICS Silhouette Score: {optics_score:.3f}\")\n",
    "    print(f\"HDBSCAN Silhouette Score: {hdbscan_score:.3f}\")\n",
    "    \n",
    "    # Return the best clustering result\n",
    "    all_labels = [kmeans_labels, dbscan_labels, hierarchical_labels, birch_labels, agglomerative_labels, optics_labels, hdbscan_labels]\n",
    "    best_labels = max(all_labels, key=lambda x: evaluate_clustering(reduced_features, x))\n",
    "    return best_labels, reduced_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Clustering Results\n",
    "\n",
    "This section provides functions for visualizing clustering results and understanding the distribution of sequences across clusters using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(features, labels, title):\n",
    "    # Use PCA to reduce features to 2 dimensions for plotting\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_features = pca.fit_transform(features)\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label, color in zip(unique_labels, colors):\n",
    "        if label == -1:  # Noise in DBSCAN or HDBSCAN\n",
    "            plt.scatter(\n",
    "                pca_features[labels == label, 0],\n",
    "                pca_features[labels == label, 1],\n",
    "                c='gray',\n",
    "                marker='x',\n",
    "                label='Noise'\n",
    "            )\n",
    "        else:\n",
    "            plt.scatter(\n",
    "                pca_features[labels == label, 0],\n",
    "                pca_features[labels == label, 1],\n",
    "                c=[color],\n",
    "                label=f'Cluster {label}'\n",
    "            )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(loc='best', bbox_to_anchor=(1.05, 1), title=\"Clusters\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_dendrogram(linkage_matrix, title):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    dendrogram(linkage_matrix, truncate_mode='lastp', p=30, show_leaf_counts=True, leaf_rotation=45, leaf_font_size=12)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Cluster Size')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_all_clusters(features, labels_dict):\n",
    "    for method, labels in labels_dict.items():\n",
    "        plot_clusters(features, labels, f\"{method} Clustering\")\n",
    "\n",
    "# Example usage in your code:\n",
    "\n",
    "file_path = output_merge_path  # Replace with your FASTA file path in the first block\n",
    "sequences = load_sequences(file_path)\n",
    "best_labels, reduced_features = cluster_sequences(sequences)\n",
    "# Generate labels_dict after clustering\n",
    "labels_dict = {\n",
    "    \"K-means\": kmeans_clustering(reduced_features, n_clusters=10),\n",
    "    \"Classical DBSCAN\": dbscan_clustering(reduced_features, eps=0.5, min_samples=5),\n",
    "    \"Hierarchical\": hierarchical_clustering(reduced_features, n_clusters=10),\n",
    "    \"Birch\": birch_clustering(reduced_features, n_clusters=10),\n",
    "    \"Agglomerative\": agglomerative_clustering(reduced_features, n_clusters=10),\n",
    "    \"OPTICS\": optics_clustering(reduced_features, min_samples=5, xi=0.05, min_cluster_size=int(0.05 * len(sequences))),\n",
    "    \"HDBSCAN\": hdbscan_clustering(reduced_features, min_cluster_size=5, min_samples=5),\n",
    "}\n",
    "plot_dendrogram(linkage(reduced_features, method='ward'), \"Hierarchical Clustering Dendrogram\")\n",
    "# Visualize all clustering results\n",
    "visualize_all_clusters(reduced_features, labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Clustered Sequences by Method and Selecting the Best Clustering Method for Consensus Sequences\n",
    "\n",
    "- All cluster files are saved in separate directories based on the clustering method used. Each file contains the sequences grouped by their respective clusters.\n",
    "- The best-performing clustering method (often K-means, as it works well in most cases) is used as an example in the following analysis blocks to generate sequence consensus and representation.\n",
    "- This ensures that the most accurate and representative sequences are captured for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustered sequences to files\n",
    "def save_cluster_sequences(sequences, labels_dict, output_dir, original_headers):\n",
    "    for method_name, labels in labels_dict.items():\n",
    "        # Create method-specific directory\n",
    "        method_dir = os.path.join(output_dir, method_name)\n",
    "        os.makedirs(method_dir, exist_ok=True)\n",
    "        \n",
    "        # Group sequences by cluster\n",
    "        clusters = {}\n",
    "        for seq, label, header in zip(sequences, labels, original_headers):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append((seq, header))\n",
    "        \n",
    "        # Save sequences for each cluster\n",
    "        for label, cluster_seqs in clusters.items():\n",
    "            cluster_file = os.path.join(method_dir, f\"cluster_{label}.fasta\")\n",
    "            with open(cluster_file, 'w') as f:\n",
    "                for seq, header in cluster_seqs:\n",
    "                    f.write(f\"{header}\\n{seq}\\n\")\n",
    "        \n",
    "        print(f\"Saved {len(clusters)} clusters for {method_name} in {method_dir}\")\n",
    "\n",
    "# Use the function with the labels dictionary\n",
    "labels_dict = {\n",
    "    \"K-means\": kmeans_clustering(reduced_features, n_clusters=10),\n",
    "    \"Classical DBSCAN\": dbscan_clustering(reduced_features, eps=0.5, min_samples=5),\n",
    "    \"Hierarchical\": hierarchical_clustering(reduced_features, n_clusters=10),\n",
    "    \"Birch\": birch_clustering(reduced_features, n_clusters=10),\n",
    "    \"Agglomerative\": agglomerative_clustering(reduced_features, n_clusters=10),\n",
    "    \"OPTICS\": optics_clustering(reduced_features, min_samples=5, xi=0.05, min_cluster_size=int(0.05 * len(sequences))),\n",
    "    \"HDBSCAN\": hdbscan_clustering(reduced_features, min_cluster_size=5, min_samples=5)\n",
    "}\n",
    "\n",
    "# Assuming you have a list of original headers\n",
    "original_headers = [f\">Sample_{i}\" for i in range(len(sequences))]\n",
    "\n",
    "# Make sure to replace \"output_clusters\" with your desired output directory path.\n",
    "save_cluster_sequences(sequences, labels_dict, \"output_clusters\", original_headers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representative Sequence Retrieval:\n",
    "### **1. Finding the Medoid of Clusters for the Best Represented Sequence (Minimal Center)**\n",
    "\n",
    "This section focuses on identifying the **medoid** of clusters to select the best-represented sequence within each cluster. The **K-Means** clustering method is chosen due to its effectiveness in grouping sequences and its ability to minimize outlier noise.\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Centroid**:\n",
    "  - Represents the geometric center of a cluster.\n",
    "  - Calculated as the average of all data points within the cluster.\n",
    "  - Note: The centroid is not guaranteed to be an actual data point.\n",
    "\n",
    "- **Medoid**:\n",
    "  - A data point within the cluster that is most centrally located.\n",
    "  - Defined as the point that minimizes the sum of distances to all other points in the cluster.\n",
    "  - Considered the best-represented sequence as it is an actual member of the cluster.\n",
    "\n",
    "#### Why Medoid over Centroid?\n",
    "- While centroids are useful for understanding the general location of a cluster, they do not necessarily correspond to real sequences.\n",
    "- Medoids, being actual data points, are more representative and interpretable for downstream analysis like simulations or functional annotations. This approach ensures the selection of sequences that are not only central to their clusters but also real and relevant for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the medoid\n",
    "def compute_medoid(cluster_data):\n",
    "    distances = pairwise_distances(cluster_data)  # Compute pairwise distances\n",
    "    medoid_index = np.argmin(distances.sum(axis=0))  # Find the point with the smallest total distance\n",
    "    return medoid_index  # Return the index of the medoid\n",
    "\n",
    "# Function to compute medoids and map back to original data\n",
    "def medoid_seq_with_data(features, labels, sequences):\n",
    "    unique_labels = np.unique(labels)\n",
    "    medoids = {}\n",
    "    medoid_sequences = {}\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label == -1:  # Skip noise points - as they don't have a cluster\n",
    "            continue\n",
    "        # Get the indices of points in the current cluster\n",
    "        cluster_indices = np.where(labels == label)[0]\n",
    "        cluster_points = features[cluster_indices]\n",
    "        \n",
    "        # Get the medoid index within the cluster\n",
    "        medoid_index_within_cluster = compute_medoid(cluster_points)\n",
    "        medoid_index = cluster_indices[medoid_index_within_cluster]  # Map back to the original index\n",
    "        \n",
    "        # Save the medoid feature and corresponding sequence\n",
    "        medoids[label] = features[medoid_index]\n",
    "        medoid_sequences[label] = sequences[medoid_index]\n",
    "    \n",
    "    return medoids, medoid_sequences\n",
    "\n",
    "# Example with K-means labels (best in most cases), you can replace it with any other clustering method's labels by changing the dict index.\n",
    "medoids, medoid_sequences = medoid_seq_with_data(reduced_features, labels_dict[\"K-means\"], sequences) \n",
    "\n",
    "# Print medoid sequences\n",
    "print(\"\\nMedoid Sequences:\")\n",
    "for cluster_id, medoid_sequence in medoid_sequences.items():\n",
    "    print(f\"Cluster {cluster_id}: {medoid_sequence}\")\n",
    "    \n",
    "# Save medoid sequences to a file in a new folder called representative_seq_mediods\n",
    "# Replace with your desired output directory\n",
    "os.makedirs(\"representative_sequences\", exist_ok=True)\n",
    "with open(\"representative_sequences/medoid_sequences.fasta\", 'w') as f:\n",
    "    for cluster_id, medoid_sequence in medoid_sequences.items():\n",
    "        f.write(f\">{cluster_id}\\n{medoid_sequence}\\n\")\n",
    "print(\"\\nMedoid sequences saved to representative_sequences/medoid_sequences.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Deriving the Optimal Consensus Sequence to Represent Each Cluster**\n",
    "\n",
    "- **Multiple Sequence Alignment (MSA):**  \n",
    "  Perform multiple sequence alignment for all sequences within each cluster using a robust MSA tool. This step ensures accurate alignment and highlights conserved residues critical to the biological function or structural integrity of the sequences.\n",
    "\n",
    "- **Consensus Sequence Generation:**  \n",
    "  Based on the aligned sequences, compute the consensus sequence that best represents the cluster. The consensus sequence reflects the most frequent or conserved residues at each position, providing a generalized representation of the cluster's functional and structural properties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CLUSTAL Alignment for Clustered Sequences**\n",
    "\n",
    "- **Sequence Alignment with CLUSTAL**:  \n",
    "  Use the CLUSTAL tool to perform multiple sequence alignment (MSA) on sequences within each cluster. This ensures accurate alignment of residues, facilitating the identification of conserved regions and patterns.\n",
    "\n",
    "- **Output Format**:  \n",
    "  Save the alignment results in a `.aln` file format, which is a standard format for representing aligned sequences and is compatible with various bioinformatics tools for further analysis.\n",
    "\n",
    "- **Purpose**:  \n",
    "  - Highlight conserved regions within the cluster to identify key functional or structural motifs.  \n",
    "  - Generate high-quality alignment data for consensus sequence derivation or downstream analysis.\n",
    "\n",
    "**Note:**\n",
    "- Ensure the sequences are cleaned and formatted properly (e.g., consistent headers) before alignment.\n",
    "- We use Biopython's CLUSTALIO wrapper for python to perform our analysis. You can use other alignment algorithms like MUSCLE. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustal_cluster(input_dir, output_dir, clustalo_path=\"clustalo\"):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all FASTA files in the input directory\n",
    "    fasta_files = glob.glob(os.path.join(input_dir, \"*.fasta\"))\n",
    "    \n",
    "    if not fasta_files:\n",
    "        print(\"No FASTA files found in the input directory.\")\n",
    "        return\n",
    "    \n",
    "    # Iterate over each FASTA file and perform alignment\n",
    "    for fasta_file in fasta_files:\n",
    "        cluster_name = os.path.splitext(os.path.basename(fasta_file))[0]\n",
    "        output_file = os.path.join(output_dir, f\"{cluster_name}_aligned.aln\")\n",
    "        \n",
    "        # Create the Clustal Omega command\n",
    "        clustalomega_cline = ClustalOmegaCommandline(\n",
    "            cmd=clustalo_path,\n",
    "            infile=fasta_file,\n",
    "            outfile=output_file,\n",
    "            outfmt = 'clustal',  \n",
    "            verbose=True,\n",
    "            auto=True\n",
    "        )\n",
    "        \n",
    "        # Print and run the command\n",
    "        print(f\"Running Clustal Omega for {fasta_file}...\")\n",
    "        try:\n",
    "            stdout, stderr = clustalomega_cline()\n",
    "            print(f\"Alignment completed for {fasta_file}. Output saved to {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fasta_file}: {e}\")\n",
    "\n",
    "# Define the input and output directories\n",
    "input_dir = \"output_clusters/K-means\"  # Path to the directory containing K-means clustered FASTA files - U can change what clustering you want to use\n",
    "output_dir = \"consensus_sequences/alignments\"    # Directory to save the aligned cluster sequences\n",
    "\n",
    "# Run the Clustal Omega alignment\n",
    "clustal_cluster(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generating Consensus Sequences**\n",
    "\n",
    "- **Consensus Sequence Derivation**:  \n",
    "  Utilize Biopython's `SummaryInfo` module to generate a consensus sequence from the aligned sequences. The consensus sequence represents the most frequent residue at each position, ensuring it reflects the key features of the aligned cluster.\n",
    "\n",
    "- **Threshold Setting**:  \n",
    "  Apply a consensus threshold of **0.7**, meaning that a residue must appear in at least 70% of the sequences at a given position to be included in the consensus. Positions with lower agreement will be represented with an ambiguous character (e.g., \"X\" or \"N\") in our case as \"N\". Changes can be made to the threshold and ambiguous character according to the needs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def consensus_sequences(msa_dir, output_file):\n",
    "    # Find all MSA files (aligned FASTA) in the directory\n",
    "    msa_files = glob.glob(os.path.join(msa_dir, \"*.aln\"))\n",
    "    \n",
    "    if not msa_files:\n",
    "        print(\"No MSA files found in the input directory.\")\n",
    "        return\n",
    "    \n",
    "    # Open the output file for writing consensus sequences\n",
    "    with open(output_file, \"w\") as output_handle:\n",
    "        for msa_file in msa_files:\n",
    "            try:\n",
    "                # Read the MSA file\n",
    "                alignment = AlignIO.read(msa_file, \"clustal\")\n",
    "                \n",
    "                # Check if alignment contains sequences\n",
    "                if not alignment:\n",
    "                    print(f\"Skipping empty or invalid file: {msa_file}\")\n",
    "                    continue\n",
    "                \n",
    "                # Compute the consensus sequence\n",
    "                summary_info = SummaryInfo(alignment)\n",
    "                consensus = summary_info.dumb_consensus(threshold=0.7, ambiguous=\"N\") # Adjust threshold and ambiguous character as needed\n",
    "                \n",
    "                # Extract the cluster name from the file name\n",
    "                cluster_name = os.path.splitext(os.path.basename(msa_file))[0]\n",
    "                \n",
    "                # Write the consensus sequence to the output file\n",
    "                output_handle.write(f\">{cluster_name}_consensus\\n{consensus}\\n\")\n",
    "                print(f\"Consensus sequence generated for {msa_file} and saved.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {msa_file}: {e}\")\n",
    "    \n",
    "    print(f\"All consensus sequences saved to {output_file}.\")\n",
    "    \n",
    "msa_dir = \"consensus_sequences/alignments\"  # Directory containing the aligned cluster FASTA files\n",
    "\n",
    "# Replace with your desired output file path\n",
    "output_file = \"consensus_sequences/cluster_consensus_sequences.fasta\"  # Path to save the consensus sequences\n",
    "\n",
    "consensus_sequences(msa_dir, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
